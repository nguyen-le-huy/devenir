# RAG System Workflow - Devenir Fashion AI

T√†i li·ªáu n√†y m√¥ t·∫£ chi ti·∫øt workflow c·ªßa h·ªá th·ªëng RAG (Retrieval-Augmented Generation) ƒë·ªÉ t∆∞ v·∫•n s·∫£n ph·∫©m, size v√† h·ªó tr·ª£ kh√°ch h√†ng.

---

## Ki·∫øn tr√∫c T·ªïng quan

```mermaid
flowchart TB
    subgraph Client["üñ•Ô∏è Client"]
        CW[ChatWindow.jsx]
        CM[ChatMessage.jsx]
    end
    
    subgraph API["üîå API Layer"]
        CR[chatRoutes.js]
    end
    
    subgraph Core["‚öôÔ∏è Core"]
        RS[RAGService.js]
        LP[LLMProvider.js]
        ConvMgr[conversation-manager.js]
    end
    
    subgraph Orchestrators["üéõÔ∏è Orchestrators"]
        IC[intent-classifier.js]
        HCI[hybridClassifyIntent]
        QID[quickIntentDetection]
    end
    
    subgraph Specialized["üéØ Specialized Services"]
        PA[product-advisor.service.js]
        SA[size-advisor.service.js]
        OL[order-lookup.service.js]
        SM[style-matcher.service.js]
        PF[policy-faq.service.js]
        AC[add-to-cart.service.js]
    end
    
    subgraph Retrieval["üîç Retrieval"]
        VS[vector-search.service.js]
        RR[reranking.service.js]
    end
    
    subgraph Generation["‚ú® Generation"]
        RG[response-generator.js]
        PB[prompt-builder.js]
    end
    
    subgraph External["‚òÅÔ∏è External Services"]
        OAI[OpenAI API]
        PC[Pinecone]
        CO[Cohere Rerank]
    end
    
    CW --> CR
    CR --> RS
    RS --> IC
    RS --> ConvMgr
    IC --> HCI
    HCI --> QID
    HCI --> LP
    IC --> PA
    IC --> SA
    IC --> OL
    IC --> SM
    IC --> PF
    IC --> AC
    PA --> VS
    PA --> RR
    PA --> RG
    SA --> VS
    RG --> PB
    LP --> OAI
    VS --> PC
    RR --> CO
```

---

## Lu·ªìng x·ª≠ l√Ω chi ti·∫øt

### 1Ô∏è‚É£ User g·ª≠i tin nh·∫Øn

```
User: "t√¨m √°o polo m√†u tr·∫Øng"
     ‚Üì
ChatWindow.jsx ‚Üí POST /api/chat
     ‚Üì
chatRoutes.js ‚Üí RAGService.chat(userId, message, history)
```

### 2Ô∏è‚É£ Ph√¢n lo·∫°i Intent (Hybrid Approach)

```javascript
// intent-classifier.js
hybridClassifyIntent(message, conversationHistory)
     ‚Üì
1. quickIntentDetection() - Keyword-based (fast)
   - "policy_faq" + confidence >= 0.7 ‚Üí Return immediately
   - "add_to_cart" + confidence >= 0.8 ‚Üí Return immediately
     ‚Üì
2. classifyIntent() - LLM-based (accurate)
   - Uses conversation history for context
   - Returns JSON: { intent, confidence, extracted_info }
     ‚Üì
3. Compare & decide
   - If LLM confidence < 0.6 ‚Üí Use keyword result
   - Otherwise ‚Üí Use LLM result
```

**Intent Types:**

| Intent | M√¥ t·∫£ | Trigger Keywords |
|--------|-------|------------------|
| `product_advice` | T∆∞ v·∫•n s·∫£n ph·∫©m | t√¨m, mu·ªën, c·∫ßn, g·ª£i √Ω, s·∫£n ph·∫©m |
| `size_recommendation` | T∆∞ v·∫•n size | size, chi·ªÅu cao, c√¢n n·∫∑ng, form |
| `style_matching` | Ph·ªëi ƒë·ªì | ph·ªëi, mix, match, outfit |
| `order_lookup` | Tra c·ª©u ƒë∆°n h√†ng | ƒë∆°n h√†ng, tracking, theo d√µi |
| `policy_faq` | Ch√≠nh s√°ch | payment, shipping, ƒë·ªïi tr·∫£ |
| `add_to_cart` | Th√™m v√†o gi·ªè | th√™m v√†o bag, add to cart, mua |
| `general` | Chung | (fallback) |

### 3Ô∏è‚É£ Parallel Processing

```javascript
// RAGService.js - T·ªëi ∆∞u performance
const [intentResult, context] = await Promise.all([
    hybridClassifyIntent(message, conversationHistory),
    conversationManager.getContext(userId, conversationHistory)
]);
```

### 4Ô∏è‚É£ Route ƒë·∫øn Service ph√π h·ª£p

```javascript
// RAGService.js
switch (intent) {
    case 'product_advice':    ‚Üí productAdvice(message, context)
    case 'size_recommendation': ‚Üí sizeRecommendation(message, extracted_info, context)
    case 'style_matching':    ‚Üí styleMatcher(message, context)
    case 'order_lookup':      ‚Üí orderLookup(message, extracted_info, userId)
    case 'policy_faq':        ‚Üí policyFAQ(message, extracted_info)
    case 'add_to_cart':       ‚Üí handleAddToCart(message, extracted_info, context)
    default:                  ‚Üí General help message
}
```

---

## Chi ti·∫øt t·ª´ng Module

### üìÅ **core/**

| File | Ch·ª©c nƒÉng |
|------|-----------|
| `RAGService.js` | Entry point - ƒëi·ªÅu ph·ªëi to√†n b·ªô flow, parallel processing |
| `VectorStore.js` | K·∫øt n·ªëi Pinecone vector database |
| `LLMProvider.js` | Wrapper cho OpenAI API v·ªõi c√°c methods: `chatCompletion()`, `jsonCompletion()`, `fastCompletion()`, `embed()` |

**LLMProvider Methods:**

```javascript
// Chat completion (GPT-4o-mini)
await llmProvider.chatCompletion(messages, { temperature: 0.3, maxTokens: 800 })

// JSON response (fast model)
await llmProvider.jsonCompletion(messages, { temperature: 0.1 })

// Embeddings (3-small, 1536 dimensions)
await llmProvider.embed(text)
await llmProvider.embedBatch(texts)
```

---

### üìÅ **orchestrators/**

| File | Ch·ª©c nƒÉng |
|------|-----------|
| `intent-classifier.js` | Hybrid classification: Keyword fallback + LLM |
| `conversation-manager.js` | Qu·∫£n l√Ω context h·ªôi tho·∫°i, l∆∞u ChatLog v√†o MongoDB |

**Keyword-based Intent Detection:**

```javascript
quickIntentDetection(message)
// Returns: { intent: 'product_advice', confidence: 0.6 }

// Priority keywords (bypass LLM):
- policy_faq: payment, shipping, crypto, payos, nowpayments
- add_to_cart: th√™m v√†o bag, add to cart, mua ngay
```

---

### üìÅ **specialized/**

#### 1. `product-advisor.service.js`

**Flow: Vector Search ‚Üí Color Filter ‚Üí Rerank ‚Üí Generate**

```javascript
async productAdvice(query, context) {
    // 1. Enrich short queries or queries with referring keywords ("n√†y", "ƒë√≥", "this")
    //    with conversation context (up to 100 chars, extracting ALL product names).
    // 2. Vector search (Pinecone, topK: 50) with Enriched Query.
    // 3. Color detection (VI ‚Üí EN mapping).
    // 4. MongoDB query for color variants.
    // 5. Rerank v·ªõi Cohere (d√πng **Enriched Query** ƒë·ªÉ gi·ªØ context s·∫£n ph·∫©m, top 10).
    // 6. Build context text (Priority: Color Match -> Vector Match).
    // 7. Generate response (OpenAI).
    // 8. Return: { answer, sources, suggested_products }
}
```

**Color Detection:**

```javascript
// Vietnamese ‚Üí English mapping
VI_TO_EN_COLORS = {
    'tr·∫Øng': 'white',
    'ƒëen': 'black',
    'ƒë·ªè r∆∞·ª£u': 'wine red',
    ...
}

// Compound colors support
COMPOUND_COLORS = ['wine red', 'navy blue', 'dusty pink', ...]
```

#### 2. `size-advisor.service.js`

**Flow: Product Lookup ‚Üí Size Chart ‚Üí LLM Recommendation**

```javascript
async sizeRecommendation(query, extractedInfo, context) {
    // 1. Get product from context or vector search
    // 2. Check Free Size / One Size
    // 3. Build size chart prompt
    // 4. LLM generates recommendation (JSON)
    // 5. Return: { answer, size_recommendation, suggested_action }
}
```

**Size Chart (Hardcoded):**

```
XS:   < 1m60, < 50kg
S:    1m60 - 1m65, 50 - 60kg
M:    1m65 - 1m70, 60 - 70kg
L:    1m70 - 1m75, 70 - 80kg
XL:   1m75 - 1m80, 80 - 90kg
XXL:  1m80 - 1m85, 90 - 100kg
```

**Suggested Action (Add to Cart Button):**

```javascript
suggested_action: {
    type: 'add_to_cart',
    prompt: 'B·∫°n c√≥ mu·ªën th√™m s·∫£n ph·∫©m n√†y v√†o gi·ªè h√†ng kh√¥ng?',
    product: { _id, name, variantId, mainImage, price },
    variant_id: '...'
}
```

#### 3. `order-lookup.service.js`

**Flow: Query Classification ‚Üí Order Search ‚Üí Format Response**

```javascript
async orderLookup(query, extractedInfo, userId) {
    // 1. LLM classify query type: list_all | specific | latest
    // 2. Route to handler:
    //    - list_all ‚Üí handleListAllOrders(userId)
    //    - latest ‚Üí handleLatestOrder(userId)
    //    - specific ‚Üí handleSpecificOrder({ order_number, phone, email })
    // 3. Format order details with status translation
}
```

**Order Status Translation:**

```javascript
statusMap = {
    'pending': 'Ch·ªù thanh to√°n',
    'paid': 'ƒê√£ thanh to√°n',
    'shipped': 'ƒêang giao h√†ng',
    'delivered': 'ƒê√£ giao th√†nh c√¥ng',
    'cancelled': 'ƒê√£ h·ªßy'
}
```

#### 4. `policy-faq.service.js`

**Flow: Keyword Detection ‚Üí Static Response**

```javascript
async policyFAQ(query, extractedInfo) {
    // No LLM needed - uses hardcoded policy info
    
    PAYMENT_INFO = {
        methods: ['PayOS', 'NowPayments (Crypto)']
    }
    
    SHIPPING_INFO = {
        options: [
            { name: 'Standard', price: 'FREE', time: '2-3 ng√†y' },
            { name: 'Next day', price: '$5', time: '1 ng√†y' },
            { name: 'Nominated', price: '$10', time: 'Ch·ªçn ng√†y' }
        ]
    }
    
    RETURN_POLICY = {
        period: '30 ng√†y',
        conditions: ['Ch∆∞a s·ª≠ d·ª•ng', 'C√≤n tag', 'C√≥ h√≥a ƒë∆°n']
    }
}
```

#### 5. `style-matcher.service.js`

G·ª£i √Ω ph·ªëi ƒë·ªì theo style/d·ªãp.

#### 6. `add-to-cart.service.js`

**Flow: Context Extraction ‚Üí Product Lookup ‚Üí Confirmation**

```javascript
async handleAddToCart(query, extractedInfo, context) {
    // 1. Find product from conversation context (regex: **ProductName**)
    // 2. Fallback to vector search
    // 3. Get first available variant
    // 4. Return suggested_action for confirmation
}
```

---

### üìÅ **retrieval/**

| File | Ch·ª©c nƒÉng |
|------|-----------|
| `vector-search.service.js` | Pinecone vector similarity search |
| `reranking.service.js` | Cohere Rerank API |

**Vector Search:**

```javascript
searchProducts(query, { topK: 50 })
// ‚Üí Pinecone query with embedding
// ‚Üí Returns: [{ id, score, metadata: { product_id, product_name, proposition_text } }]
```

**Reranking:**

```javascript
rerankDocuments(query, documents, topN = 10)
// ‚Üí Cohere rerank-multilingual-v3.0
// ‚Üí Returns: [{ index, relevance_score }]
```

---

### üìÅ **generation/**

| File | Ch·ª©c nƒÉng |
|------|-----------|
| `prompt-builder.js` | X√¢y d·ª±ng prompts cho c√°c use cases |
| `response-generator.js` | G·ªçi LLMProvider ƒë·ªÉ generate |

**CoVe Prompt (Chain of Verification):**

```javascript
buildCoVePrompt(context, conversationHistory)
// Quy t·∫Øc:
// - CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ [Context]
// - KH√îNG b·ªãa ƒë·∫∑t
// - KH√îNG emoji
// - Gi√° d·∫°ng $XXX
// - D√πng **bold** cho t√™n s·∫£n ph·∫©m
```

---

### üìÅ **embeddings/**

| File | Ch·ª©c nƒÉng |
|------|-----------|
| `embedding.service.js` | OpenAI embeddings (text-embedding-3-small) |
| `proposition.service.js` | Chia product info th√†nh chunks |

---

## Example Flows

### Flow 1: T∆∞ v·∫•n S·∫£n ph·∫©m

```
User: "t√¨m √°o polo m√†u wine red"
     ‚Üì
1. hybridClassifyIntent ‚Üí "product_advice" (confidence: 0.8)
     ‚Üì
2. productAdvice():
   2a. searchProducts("t√¨m √°o polo m√†u wine red", topK: 50)
   2b. findColorInQuery() ‚Üí { vi: "wine red", en: "wine red" }
   2c. MongoDB: ProductVariant.find({ color: /wine red/i })
   2d. rerankDocuments(query, propositions, top: 10)
   2e. Build context with color-matched products first
   2f. generateResponse(query, context)
     ‚Üì
3. Response:
   {
     answer: "D·∫° c√≥ b·∫°n, m√¨nh c√≥ **Cotton Polo Shirt** m√†u Wine Red...",
     sources: [...],
     suggested_products: [{_id, name, variantId, mainImage, price}]
   }
```

### Flow 2: T∆∞ v·∫•n Size

```
User: "size M c√≥ v·ª´a kh√¥ng, cao 1m70 n·∫∑ng 65kg"
     ‚Üì
1. hybridClassifyIntent ‚Üí "size_recommendation"
   extracted_info: { height: 170, weight: 65 }
     ‚Üì
2. sizeRecommendation():
   2a. Get product from context or search
   2b. Check Free Size ‚Üí If yes, return immediately
   2c. Build prompt with size chart
   2d. LLM ‚Üí JSON: { recommended_size: "M", reason: "..." }
   2e. Get variants of recommended size
     ‚Üì
3. Response v·ªõi suggested_action:
   {
     answer: "V·ªõi chi·ªÅu cao 1m70 v√† c√¢n n·∫∑ng 65kg, size M s·∫Ω v·ª´a...",
     suggested_action: {
       type: "add_to_cart",
       variant_id: "...",
       product: {...}
     }
   }
     ‚Üì
4. Client renders "C√≥, th√™m v√†o gi·ªè" / "Kh√¥ng, c·∫£m ∆°n" buttons
```

### Flow 3: Tra c·ª©u ƒê∆°n h√†ng

```
User: "ƒë∆°n h√†ng c·ªßa t√¥i"
     ‚Üì
1. hybridClassifyIntent ‚Üí "order_lookup"
     ‚Üì
2. orderLookup(query, extractedInfo, userId):
   2a. LLM classify ‚Üí "list_all"
   2b. Check userId (logged in)
   2c. handleListAllOrders(userId)
   2d. Order.find({ user: userId }).sort({ createdAt: -1 }).limit(5)
     ‚Üì
3. Response:
   {
     answer: "ƒê√¢y l√† 5 ƒë∆°n h√†ng g·∫ßn nh·∫•t c·ªßa b·∫°n:\n\n1. #AB12CD34...",
     orders: [...]
   }
```

### Flow 4: Policy FAQ

```
User: "thanh to√°n b·∫±ng crypto ƒë∆∞·ª£c kh√¥ng"
     ‚Üì
1. quickIntentDetection ‚Üí "policy_faq" (confidence: 0.8)
   ‚ö° Bypass LLM (high-confidence keyword match)
     ‚Üì
2. policyFAQ():
   - isPaymentQuery = true (keyword: "crypto")
   - Return hardcoded payment info
     ‚Üì
3. Response:
   {
     answer: "**Ph∆∞∆°ng th·ª©c thanh to√°n t·∫°i DEVENIR:**\n\n1. PayOS...\n2. NowPayments..."
   }
```

---

## Client-side Integration

```javascript
// ChatWindow.jsx
handleSendMessage(message)
     ‚Üì
sendChatMessage(message, conversationHistory)
     ‚Üì
Receive response: { intent, answer, suggested_action, suggested_products }
     ‚Üì
ChatMessage.jsx
     ‚Üì
// Render answer with markdown
// Render product cards if suggested_products
// Render action buttons if suggested_action
     ‚Üì
handleYesClick() ‚Üí useAddToCart(variantId, quantity)
```

---

## Performance Optimizations

1. **Parallel Processing**: Intent classification + Context retrieval run in parallel
2. **Keyword Bypass**: High-confidence keywords skip LLM classification
3. **Color Cache**: Database colors cached for 1 hour
4. **Query Enrichment**: Short queries (<100 chars) or queries with referring words ("n√†y", "ƒë√≥") are enriched with product context from history.
5. **Selective Fields**: MongoDB queries use `.select()` for only needed fields

---

## Models Used

| Purpose | Model | Provider |
|---------|-------|----------|
| Chat Generation | gpt-4o-mini | OpenAI |
| Intent Classification | gpt-4o-mini (fast) | OpenAI |
| Embeddings | text-embedding-3-small | OpenAI |
| Reranking | rerank-multilingual-v3.0 | Cohere |
| Vector Database | Pinecone | Pinecone |
